{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setGPU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import h5py as h5\n",
    "from gwpy.timeseries import TimeSeries\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of the time-series data we are working with\n",
    "\n",
    "strain = TimeSeries.read('/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.4k/hdf.v1/L1/1185939456/L-L1_GWOSC_O2_4KHZ_R1-1186205696-4096.hdf5',format='hdf5.losc')\n",
    "strain.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "\n",
    "train_start = 1185939456\n",
    "num_train = 5\n",
    "train_files = [train_start + i*4096 for i in range(num_train)]\n",
    "detector = 'L1'\n",
    "x = np.array([])\n",
    "\n",
    "for file in train_files: \n",
    "    load = h5.File('/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.4k/hdf.v1/%s/%s/%s-%s_GWOSC_O2_4KHZ_R1-%s-4096.hdf5'%(detector, str(train_start), detector[0], detector, str(file)))\n",
    "    x = np.concatenate((x, load['strain']['Strain'][()]), axis=0)\n",
    "    \n",
    "    \n",
    "test_start = 1185939456\n",
    "num_test = 1\n",
    "test_files = [test_start + (num_train)*4096 + i*4096 for i in range(num_test)]\n",
    "detector = 'L1'\n",
    "y = np.array([])\n",
    "\n",
    "for file in test_files: \n",
    "    load = h5.File('/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.4k/hdf.v1/%s/%s/%s-%s_GWOSC_O2_4KHZ_R1-%s-4096.hdf5'%(detector, str(train_start), detector[0], detector, str(file)))\n",
    "    y = np.concatenate((x, load['strain']['Strain'][()]), axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val data: 16777216\n",
      "train data: 150994944\n"
     ]
    }
   ],
   "source": [
    "#Data generator not sure if I'll need this (not used currently)\n",
    "\n",
    "from data import H5Data\n",
    "import glob\n",
    "\n",
    "datadir = '/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.4k/hdf.v1/L1/1185939456/'\n",
    "files = glob.glob(datadir + \"/*\")\n",
    "files_val = files[:1]\n",
    "files_train = files[1:10]\n",
    "\n",
    "batch_size = 128\n",
    "data_train = H5Data(batch_size = batch_size,\n",
    "                    cache = None,\n",
    "                    preloading=0,\n",
    "                    primary_key='strain',\n",
    "                    secondary_key='Strain')\n",
    "data_train.set_file_names(files_train)\n",
    "data_val = H5Data(batch_size = batch_size,\n",
    "                    cache = None,\n",
    "                    preloading=0,\n",
    "                    primary_key='strain',\n",
    "                    secondary_key='Strain')\n",
    "data_val.set_file_names(files_val)\n",
    "n_val=data_val.count_data()\n",
    "n_train=data_train.count_data()\n",
    "print(\"val data:\", n_val)\n",
    "print(\"train data:\", n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (83886080, 1, 1)\n",
      "Test data shape: (100663296, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(x.reshape(-1, 1))\n",
    "X_test = scaler.transform(y.reshape(-1, 1))\n",
    "scaler_filename = \"scaler_data\"\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# reshape inputs for LSTM [samples, timesteps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "# define the autoencoder network model\n",
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = LSTM(16, activation='relu', return_sequences=True, \n",
    "              kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 1)              0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 16)             1152      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4)                 336       \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 4)              0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1, 4)              144       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1, 16)             1344      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              17        \n",
      "=================================================================\n",
      "Total params: 2,993\n",
      "Trainable params: 2,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67108864 samples, validate on 16777216 samples\n",
      "Epoch 1/100\n",
      "12544000/67108864 [====>.........................] - ETA: 14:28 - loss: 0.0053"
     ]
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "nb_epochs = 100\n",
    "batch_size = 1024\n",
    "history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\n",
    "                    validation_split=0.2).history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
